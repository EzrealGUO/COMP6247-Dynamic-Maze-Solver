{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393eb4d7",
   "metadata": {},
   "source": [
    "# Pre-Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f35f8a",
   "metadata": {},
   "source": [
    "In this part, a basic DQN architecture is implemented and tested on the self-created mazes with easy-training scale but the same designing specifications with the required problem. The details are as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829b2cd",
   "metadata": {},
   "source": [
    "#### Library Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD,Adam,RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd686f7",
   "metadata": {},
   "source": [
    "#### Self-created Maze Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590cc258",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "    [ 0,  1,  1,  1,  1,  0,  0,  0,  1,  0],\n",
    "    [ 0,  1,  0,  0,  1,  1,  1,  1,  1,  0],\n",
    "    [ 0,  1,  0,  0,  0,  1,  0,  0,  1,  0],\n",
    "    [ 0,  1,  0,  1,  0,  1,  0,  0,  1,  0],\n",
    "    [ 0,  1,  0,  1,  0,  1,  0,  0,  0,  0],\n",
    "    [ 0,  1,  1,  1,  0,  1,  1,  1,  1,  0],\n",
    "    [ 0,  1,  0,  0,  0,  1,  0,  0,  0,  0],\n",
    "    [ 0,  1,  0,  0,  0,  1,  1,  1,  1,  0],\n",
    "    [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "])\n",
    "print(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11502da8",
   "metadata": {},
   "source": [
    "Within this static (10,10) maze, thet agent need to travel from (1,1) to (9,9), avoiding all the walls (represented by '0') in its journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f34d6a",
   "metadata": {},
   "source": [
    "#### Action Definition\n",
    "Include travel trace mark, action encoding, and a exploration factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85446415",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_mark = 2\n",
    "agent_mark = 2\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_list = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_list)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a2b4b",
   "metadata": {},
   "source": [
    "#### Basic Rules Setting\n",
    "Return +10 for reaching the destination, in order to positively award the arrival greatly and only;\n",
    "\n",
    "Return -10 for entering the regions of wall, in order to stop the illegal action with severe punishment; \n",
    "\n",
    "Return -0.01 for every single action, in order to encourage the agent to seek the shortest routine;\n",
    "\n",
    "Return -0.1 for revisiting a experienced block, in order to prevent the agent from struggling around;\n",
    "\n",
    "Eliminate actions hitting the walls and boundaries;\n",
    "\n",
    "Reset game when sum of rewards exceeding (10 * 10) * -0.01;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNmaze(object):\n",
    "    # Maze Class Initialization\n",
    "    def __init__(self, maze, agent=(1,1)):\n",
    "        self.maze = np.array(maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        self.target = (8,8)\n",
    "        self.path = [(r,c) for r in range(nrows) for c in range(ncols) if self.maze[r,c] == 1]\n",
    "        self.reset(agent)\n",
    "\n",
    "    # Reset Agent State \n",
    "    def reset(self, agent):\n",
    "        self.agent = agent\n",
    "        self.maze = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = agent\n",
    "        self.maze[row, col] = agent_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.threshold = -0.01 * self.maze.shape[0]**2\n",
    "        self.reward_sum = 0\n",
    "        self.visited = set()\n",
    "        \n",
    "    # Update State Corresponding to Action Encoding and Validation\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = agent_row, agent_col, mode = self.state\n",
    "        if self.maze[agent_row, agent_col] > 0:\n",
    "            self.visited.add((agent_row, agent_col))  # mark visited cell\n",
    "        valid_actions = self.valid_actions()\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            elif action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:\n",
    "            mode = 'invalid'\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    # Update Reward based on Pre-defined Rules\n",
    "    def get_reward(self):\n",
    "        agent_row, agent_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if agent_row == 8 and agent_col == 8:\n",
    "            return 10.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (agent_row, agent_col) in self.visited:\n",
    "            return -0.1\n",
    "        if mode == 'invalid':\n",
    "            return -10\n",
    "        if mode == 'valid':\n",
    "            return -0.01\n",
    "        \n",
    "    # Record Reflections to Action\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.reward_sum += reward\n",
    "        status = self.travel_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    # Input Environment Information as Vector\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    # Visualization Encodes\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 0.01\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = agent_mark\n",
    "        return canvas\n",
    "\n",
    "    # Judging Travel Status \n",
    "    def travel_status(self):\n",
    "        if self.reward_sum < self.threshold:\n",
    "            return 'lose'\n",
    "        agent_row, agent_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if agent_row == 8 and agent_col == 8:\n",
    "            return 'win'\n",
    "        return 'not_over'\n",
    "\n",
    "    # Validation for Actions: Agent is not allowed to go into the walls\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        \n",
    "        if row == 1:\n",
    "            actions.remove(1)\n",
    "        elif row>1 and self.maze[row-1,col] == 0:\n",
    "            actions.remove(1)\n",
    "        if row == nrows-2:\n",
    "            actions.remove(3)\n",
    "        elif row<nrows-2 and self.maze[row+1,col] == 0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 1:\n",
    "            actions.remove(0)\n",
    "        elif col>1 and self.maze[row,col-1] == 0:\n",
    "            actions.remove(0)    \n",
    "        if col == ncols-2:\n",
    "            actions.remove(2)\n",
    "        elif col<ncols-2 and self.maze[row,col+1] == 0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13158114",
   "metadata": {},
   "source": [
    "#### Action Funtion Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Sample Visulization in Gray-scale \n",
    "def show(DQNmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = DQNmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(DQNmaze.maze)\n",
    "    for row,col in DQNmaze.visited:\n",
    "        canvas[row,col] = 2\n",
    "    agent_row, agent_col, _ = DQNmaze.state\n",
    "    canvas[agent_row, agent_col] = 2 \n",
    "    canvas[nrows-2, ncols-2] = 2\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNmaze = DQNmaze(maze)\n",
    "show(DQNmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22448d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNmaze = DQNmaze(maze)\n",
    "DQNmaze.act(DOWN)\n",
    "DQNmaze.act(DOWN)\n",
    "DQNmaze.act(DOWN)\n",
    "DQNmaze.act(DOWN)\n",
    "DQNmaze.act(DOWN)\n",
    "DQNmaze.act(DOWN)\n",
    "show(DQNmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b64e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNmaze = DQNmaze(maze)\n",
    "DQNmaze.act(RIGHT)\n",
    "DQNmaze.act(RIGHT)\n",
    "DQNmaze.act(RIGHT)\n",
    "DQNmaze.act(DOWN)\n",
    "DQNmaze.act(RIGHT)\n",
    "DQNmaze.act(DOWN)\n",
    "show(DQNmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb87cd",
   "metadata": {},
   "source": [
    "### Start Travelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d11a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Travel Function as the Core of Iterations\n",
    "# Inputs are Outputs of the Previous Step\n",
    "def travel(model, DQNmaze, agent_start):\n",
    "    DQNmaze.reset(agent_start)\n",
    "    envstate = DQNmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate \n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "        #  action, get rewards and new state\n",
    "        envstateupdate, reward, travel_status = DQNmaze.act(action)\n",
    "        if travel_status == 'win':\n",
    "            return True\n",
    "        elif travel_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18834987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Details in Eacch Iteration\n",
    "class Experience(object):\n",
    "    def __init__(self, model, limit=100, gamma=0.95):\n",
    "        self.model = model\n",
    "        self.limit = limit\n",
    "        self.gamma = gamma\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    # Store Previous Experiences as training data\n",
    "    # Parameter Limit limits the Memory Upper Bound\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, travel_over]\n",
    "        # memory[i] = episode\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.limit:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    # Get Input Data and Targets for the Next Iteration\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, travel_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.gamma * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd17e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deinfe Neural Network\n",
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f990ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Training Architecture\n",
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    limit = opt.get('limit', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Load Previous Weight File\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Environment Construction\n",
    "    dqnmaze = DQNmaze(maze)\n",
    "\n",
    "    # Initialize Experience Object and Related Records\n",
    "    experience = Experience(model, limit=limit)\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_path = len(dqnmaze.path)\n",
    "    hsize = dqnmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    # Iteration Functioning\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        agent = random.choice(dqnmaze.path)\n",
    "        dqnmaze.reset(agent)\n",
    "        travel_over = False\n",
    "\n",
    "        envstate = dqnmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not travel_over:\n",
    "            valid_actions = dqnmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "                \n",
    "            prev_envstate = envstate\n",
    "            \n",
    "            # Prediction of Next Action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "                \n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, travel_status = dqnmaze.act(action)\n",
    "            if travel_status == 'win':\n",
    "                win_history.append(1)\n",
    "                travel_over = True\n",
    "            elif travel_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                travel_over = True\n",
    "            else:\n",
    "                travel_over = False\n",
    "                \n",
    "            # Store Experience\n",
    "            episode = [prev_envstate, action, reward, envstate, travel_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "            \n",
    "            # Train Neural Network\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize:\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save Trained Model Weights and Architecture\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# Reset Time Units\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc448ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNmaze = DQNmaze(maze)\n",
    "show(DQNmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(maze)\n",
    "qtrain(model, maze, epochs=1000, limit=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Game based on Trained Model\n",
    "experience = Experience(model)\n",
    "Output = experience.getdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c037631",
   "metadata": {},
   "source": [
    "# Formal Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a1484",
   "metadata": {},
   "source": [
    "In this part, the DQN architecture is expanded and tested on the more challenging questions about large-scale static maze and a dynamic maze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8a903",
   "metadata": {},
   "source": [
    "# Static Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b265405",
   "metadata": {},
   "source": [
    "#### Environment Setting Illustration\n",
    "Load the basic information of the target maze and plot visualization for obervation use only.\n",
    "\n",
    "load_maze function constructs the whole maze environment and the get_information function returns the surrounding information of a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d644b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import read_maze\n",
    "from read_maze import load_maze\n",
    "from read_maze import get_local_maze_information as get_information\n",
    "import matplotlib.pyplot as plt\n",
    "load_maze()\n",
    "depthmap = np.load('COMP6247Maze20212022.npy')\n",
    "ax = plt.subplots(figsize = (100,100))\n",
    "plt.imshow(depthmap)\n",
    "plt.savefig('maze.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e14ebf",
   "metadata": {},
   "source": [
    "#### Action Definition and DQN Maze Class Construction\n",
    "Apply the same action definitions as before.\n",
    "\n",
    "Alter the model architecture due to unkonwn information of the whole maze.\n",
    "\n",
    "Numerically alter the reward rules to fit the new maze of large scale.\n",
    "\n",
    "Change the draw_env and the observe function as illustrating get_information to get surrounding information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f250587",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_list = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_list)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1\n",
    "class DQNmaze(object):\n",
    "    # Maze Class Initialization\n",
    "    def __init__(self, maze, agent=(1,1)):\n",
    "        self.maze = np.array(maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        self.target = (199,199)\n",
    "        self.reset(agent)\n",
    "\n",
    "    # Reset Agent State \n",
    "    def reset(self, agent):\n",
    "        self.agent = agent\n",
    "        self.maze = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = agent\n",
    "        self.state = (row, col, 'start')\n",
    "        self.threshold = -0.01 * self.maze.shape[0]**2\n",
    "        self.reward_sum = 0\n",
    "        self.visited = set()\n",
    "        \n",
    "    # Update State Corresponding to Action Encoding and Validation\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = agent_row, agent_col, mode = self.state\n",
    "        if self.maze[agent_row, agent_col] > 0:\n",
    "            self.visited.add((agent_row, agent_col))  # mark visited cell\n",
    "        valid_actions = self.valid_actions()\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            elif action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:\n",
    "            mode = 'invalid'\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    # Update Reward based on Pre-defined Rules\n",
    "    def get_reward(self):\n",
    "        agent_row, agent_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if agent_row == 199 and agent_col == 199:\n",
    "            return 50\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (agent_row, agent_col) in self.visited:\n",
    "            return -0.1\n",
    "        if mode == 'invalid':\n",
    "            return -50\n",
    "        if mode == 'valid':\n",
    "            return -0.01\n",
    "        \n",
    "    # Record Reflections to Action\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.reward_sum += reward\n",
    "        status = self.travel_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    # Input Environment Information as Vector\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    # Visualization Encodes\n",
    "    def draw_env(self):\n",
    "        row, col, valid = self.state\n",
    "        m = self.state[0]\n",
    "        n = self.state[1]\n",
    "        environment = get_information(m,n)\n",
    "        canvas = np.zeros((3,3))\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                canvas[i][j] = environment[i][j][0]\n",
    "                self.maze[row+i-1][col+j-1] = environment[i][j][0]\n",
    "        return canvas\n",
    "\n",
    "    # Judging Travel Status \n",
    "    def travel_status(self):\n",
    "        if self.reward_sum < self.threshold:\n",
    "            return 'lose'\n",
    "        agent_row, agent_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if agent_row == 199 and agent_col == 199:\n",
    "            return 'win'\n",
    "        return 'not_over'\n",
    "\n",
    "    # Validation for Actions: Agent is not allowed to go into the walls\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        \n",
    "        if row == 1:\n",
    "            actions.remove(1)\n",
    "        elif row>1 and self.maze[row-1,col] == 0:\n",
    "            actions.remove(1)\n",
    "        if row == nrows-2:\n",
    "            actions.remove(3)\n",
    "        elif row<nrows-2 and self.maze[row+1,col] == 0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 1:\n",
    "            actions.remove(0)\n",
    "        elif col>1 and self.maze[row,col-1] == 0:\n",
    "            actions.remove(0)    \n",
    "        if col == ncols-2:\n",
    "            actions.remove(2)\n",
    "        elif col<ncols-2 and self.maze[row,col+1] == 0:\n",
    "            actions.remove(2)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b671378",
   "metadata": {},
   "source": [
    "#### Travel and Training Definition\n",
    "The Input has to be changed to the Surrounding Environment of the State.\n",
    "\n",
    "This leads to no significant change to the algorithm architecture, but change the core thinking.\n",
    "\n",
    "We now aim to learn the mapping from the environment of (3,3) to the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Travel Function as the Core of Iterations\n",
    "# Inputs are Outputs of the Previous Step\n",
    "def travel(model, DQNmaze, agent_start):\n",
    "    DQNmaze.reset(agent_start)\n",
    "    envstate = DQNmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate \n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "        #  action, get rewards and new state\n",
    "        envstateupdate, reward, travel_status = DQNmaze.act(action)\n",
    "        if travel_status == 'win':\n",
    "            return True\n",
    "        elif travel_status == 'lose':\n",
    "            return False\n",
    "        \n",
    "# Define Details in Each Iteration\n",
    "class Experience(object):\n",
    "    def __init__(self, model, limit=3000, gamma=0.95):\n",
    "        self.model = model\n",
    "        self.limit = limit\n",
    "        self.gamma = gamma\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    # Store Previous Experiences as training data\n",
    "    # Parameter Limit limits the Memory Upper Bound\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, travel_over]\n",
    "        # memory[i] = episode\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.limit:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    # Get Input Data and Targets for the Next Iteration\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, travel_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if travel_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.gamma * Q_sa\n",
    "        return inputs, targets\n",
    "    \n",
    "# Deinfe Neural Network\n",
    "def build_model(lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(9, input_shape=(9,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(9))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build Training Architecture\n",
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    limit = opt.get('limit', 3000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Load Previous Weight File\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Environment Construction\n",
    "    dqnmaze = DQNmaze(maze)\n",
    "\n",
    "    # Initialize Experience Object and Related Records\n",
    "    experience = Experience(model, limit=limit)\n",
    "    win_history = []   # history of win/lose game\n",
    "    hsize = dqnmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    # Iteration Functioning\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        agent = (1,1)\n",
    "        dqnmaze.reset(agent)\n",
    "        travel_over = False\n",
    "\n",
    "        envstate = dqnmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not travel_over:\n",
    "            valid_actions = dqnmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "                \n",
    "            prev_envstate = envstate\n",
    "            \n",
    "            # Prediction of Next Action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "                \n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, travel_status = dqnmaze.act(action)\n",
    "            if travel_status == 'win':\n",
    "                win_history.append(1)\n",
    "                travel_over = True\n",
    "            elif travel_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                travel_over = True\n",
    "            else:\n",
    "                travel_over = False\n",
    "                \n",
    "            # Store Experience\n",
    "            episode = [prev_envstate, action, reward, envstate, travel_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "            \n",
    "            # Train Neural Network\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize:\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save Trained Model Weights and Architecture\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# Reset Time Units\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332145cb",
   "metadata": {},
   "source": [
    "#### Training and Testing\n",
    "Without the whole maze information, a blank maze has to be defined for the agent to travel. And the model need to be initialized with blank environemnt\n",
    "\n",
    "The Results are stored in the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.zeros((201,201))\n",
    "environ = np.zeros((3,3))\n",
    "model = build_model(environ)\n",
    "qtrain(model, maze, epochs=10000, limit=0.1*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411239c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = Experience(model)\n",
    "Environment,Action = experience.getdata()\n",
    "for i in range(Environment.shape[0]-1):\n",
    "    MazeInfo = []\n",
    "    m,n = location[i]\n",
    "    for j in len(Environment[i]):\n",
    "            if j==4:\n",
    "                MazeInfo.append(\"Location\")\n",
    "            else:\n",
    "                if (Environment[i]):\n",
    "                    MazeInfo.append(\"   Path   \")\n",
    "                else:\n",
    "                    MazeInfo.append(\"   Wall   \")\n",
    "    File = open('Solution.txt',mode='a')\n",
    "    File.writelines(['Time : ',str(i),'\\n'])\n",
    "    File.writelines(['Location : ( ',str(int(m)),', ',str(int(n)),' )\\n'])\n",
    "    File.writelines(['Environment : ',MazeInfo[0],MazeInfo[1],MazeInfo[2],'\\n'])\n",
    "    File.writelines(['                      ',MazeInfo[3],MazeInfo[4],MazeInfo[5],'\\n'])\n",
    "    File.writelines(['                      ',MazeInfo[6],MazeInfo[7],MazeInfo[8],'\\n'])\n",
    "    File.writelines(['Action : ',Action[i]])\n",
    "    File.writelines(['\\n','\\n'])\n",
    "    File.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145040b8",
   "metadata": {},
   "source": [
    "# Dynamic Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb48e9",
   "metadata": {},
   "source": [
    "#### Action Definition and DQN Maze Class Construction\n",
    "Alter the action definitions, adding wait option.\n",
    "\n",
    "Numerically alter the reward rules to fit the new actions.\n",
    "\n",
    "Change the draw_env and the observe functions encoding the Fire state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "WAIT = 4\n",
    "\n",
    "# Actions dictionary\n",
    "actions_list = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "    WAIT: 'wait',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_list)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.2\n",
    "class DQNmaze(object):\n",
    "    # Maze Class Initialization\n",
    "    def __init__(self, maze, agent=(1,1)):\n",
    "        self.maze = np.array(maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        self.target = (199,199)\n",
    "        self.reset(agent)\n",
    "\n",
    "    # Reset Agent State \n",
    "    def reset(self, agent):\n",
    "        self.agent = agent\n",
    "        self.maze = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = agent\n",
    "        self.state = (row, col, 'start')\n",
    "        self.threshold = -0.05 * self.maze.shape[0]**2\n",
    "        self.reward_sum = 0\n",
    "        self.visited = set()\n",
    "        \n",
    "    # Update State Corresponding to Action Encoding and Validation\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = agent_row, agent_col, mode = self.state\n",
    "        if self.maze[agent_row, agent_col] > 0:\n",
    "            self.visited.add((agent_row, agent_col))  # mark visited cell\n",
    "        valid_actions = self.valid_actions()\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            elif action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "            elif action == WAIT:\n",
    "                nrow = nrow\n",
    "                ncol = ncol\n",
    "                mode = 'stop'\n",
    "        else:\n",
    "            mode = 'invalid'\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    # Update Reward based on Pre-defined Rules\n",
    "    def get_reward(self):\n",
    "        agent_row, agent_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if agent_row == 199 and agent_col == 199:\n",
    "            return 100\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (agent_row, agent_col) in self.visited:\n",
    "            return -1\n",
    "        if mode == 'invalid':\n",
    "            return -100\n",
    "        if mode == 'valid':\n",
    "            return -0.01\n",
    "        if mode == 'stop':\n",
    "            return -0.1\n",
    "        \n",
    "    # Record Reflections to Action\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.reward_sum += reward\n",
    "        status = self.travel_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    # Input Environment Information as Vector\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        \n",
    "        return envstate\n",
    "\n",
    "    # Visualization Encodes\n",
    "    def draw_env(self):\n",
    "        row, col, valid = self.state\n",
    "        m = self.state[0]\n",
    "        n = self.state[1]\n",
    "        environment = get_information(m,n)\n",
    "        canvas = np.zeros((3,3))\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                canvas[i][j] = environment[i][j][0]\n",
    "                self.maze[row+i-1][col+j-1] = environment[i][j][0]\n",
    "                if canvas[i][j] and environment [i][j][1]:\n",
    "                    canvas[i][j] = 2\n",
    "                    self.maze[row+i-1][col+j-1] = 0\n",
    "        return canvas\n",
    "\n",
    "    # Judging Travel Status \n",
    "    def travel_status(self):\n",
    "        if self.reward_sum < self.threshold:\n",
    "            return 'lose'\n",
    "        agent_row, agent_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if agent_row == 199 and agent_col == 199:\n",
    "            return 'win'\n",
    "        return 'not_over'\n",
    "\n",
    "    # Validation for Actions: Agent is not allowed to go into the walls and fires\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3,4]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        \n",
    "        if row == 1:\n",
    "            actions.remove(1)\n",
    "        elif row>1 and self.maze[row-1,col] == 0:\n",
    "            actions.remove(1)\n",
    "        if row == nrows-2:\n",
    "            actions.remove(3)\n",
    "        elif row<nrows-2 and self.maze[row+1,col] == 0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 1:\n",
    "            actions.remove(0)\n",
    "        elif col>1 and self.maze[row,col-1] == 0:\n",
    "            actions.remove(0)    \n",
    "        if col == ncols-2:\n",
    "            actions.remove(2)\n",
    "        elif col<ncols-2 and self.maze[row,col+1] == 0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf81b6",
   "metadata": {},
   "source": [
    "#### Travel and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252fc96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Travel Function as the Core of Iterations\n",
    "# Inputs are Outputs of the Previous Step\n",
    "def travel(model, DQNmaze, agent_start):\n",
    "    DQNmaze.reset(agent_start)\n",
    "    envstate = DQNmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate \n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "        #  action, get rewards and new state\n",
    "        envstateupdate, reward, travel_status = DQNmaze.act(action)\n",
    "        if travel_status == 'win':\n",
    "            return True\n",
    "        elif travel_status == 'lose':\n",
    "            return False\n",
    "        \n",
    "# Define Details in Each Iteration\n",
    "class Experience(object):\n",
    "    def __init__(self, model, limit=5000, gamma=0.95):\n",
    "        self.model = model\n",
    "        self.limit = limit\n",
    "        self.gamma = gamma\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    # Store Previous Experiences as training data\n",
    "    # Parameter Limit limits the Memory Upper Bound\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, travel_over]\n",
    "        # memory[i] = episode\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.limit:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    # Get Input Data and Targets for the Next Iteration\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, travel_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if travel_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.gamma * Q_sa\n",
    "        return inputs, targets\n",
    "    \n",
    "# Deinfe Neural Network\n",
    "def build_model(lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(9, input_shape=(9,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(9))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Build Training Architecture\n",
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    limit = opt.get('limit', 5000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Load Previous Weight File\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Environment Construction\n",
    "    dqnmaze = DQNmaze(maze)\n",
    "\n",
    "    # Initialize Experience Object and Related Records\n",
    "    experience = Experience(model, limit=limit)\n",
    "    win_history = []   # history of win/lose game\n",
    "    hsize = dqnmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    # Iteration Functioning\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        agent = (1,1)\n",
    "        dqnmaze.reset(agent)\n",
    "        travel_over = False\n",
    "\n",
    "        envstate = dqnmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not travel_over:\n",
    "            valid_actions = dqnmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "                \n",
    "            prev_envstate = envstate\n",
    "            \n",
    "            # Prediction of Next Action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "                \n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, travel_status = dqnmaze.act(action)\n",
    "            if travel_status == 'win':\n",
    "                win_history.append(1)\n",
    "                travel_over = True\n",
    "            elif travel_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                travel_over = True\n",
    "            else:\n",
    "                travel_over = False\n",
    "                \n",
    "            # Store Experience\n",
    "            episode = [prev_envstate, action, reward, envstate, travel_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "            \n",
    "            # Train Neural Network\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize:\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save Trained Model Weights and Architecture\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# Reset Time Units\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.zeros((201,201))\n",
    "environ = np.zeros((3,3))\n",
    "model = build_model(environ)\n",
    "qtrain(model, maze, epochs=10000, limit=0.5*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = Experience(model)\n",
    "Environment,Action = experience.getdata()\n",
    "for i in range(Environment.shape[0]-1):\n",
    "    MazeInfo = []\n",
    "    m,n = location[i]\n",
    "    for j in len(Environment[i]):\n",
    "            if j==4:\n",
    "                MazeInfo.append(\"Location\")\n",
    "            else:\n",
    "                if (Environment[i]):\n",
    "                    if (Environment[i]==1):\n",
    "                        MazeInfo.append(\"   Path   \")\n",
    "                    else:\n",
    "                        MazeInfo.append(\"   Fire   \")\n",
    "                else:\n",
    "                    MazeInfo.append(\"   Wall   \")\n",
    "    File = open('Solution2.txt',mode='a')\n",
    "    File.writelines(['Time : ',str(i),'\\n'])\n",
    "    File.writelines(['Location : ( ',str(int(m)),', ',str(int(n)),' )\\n'])\n",
    "    File.writelines(['Environment : ',MazeInfo[0],MazeInfo[1],MazeInfo[2],'\\n'])\n",
    "    File.writelines(['                      ',MazeInfo[3],MazeInfo[4],MazeInfo[5],'\\n'])\n",
    "    File.writelines(['                      ',MazeInfo[6],MazeInfo[7],MazeInfo[8],'\\n'])\n",
    "    File.writelines(['Action : ',Action[i]])\n",
    "    File.writelines(['\\n','\\n'])\n",
    "    File.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
